## Features

- **CombU Activation**: A custom activation function combining ReLU, ELU, and NLReLU.
- **Multi-Head Self-Attention**: Allows the model to focus on different parts of the input simultaneously.
- **Transformer Architecture**: Implements the full Transformer architecture with multiple layers.
- **Positional Encoding**: Adds information about token positions in the sequence.
- **MLX Framework**: Utilizes Apple's MLX for efficient computation on Apple Silicon.